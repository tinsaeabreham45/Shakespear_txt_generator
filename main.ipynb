{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4ea3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0094e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = f'./data/shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77edd5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us\n"
     ]
    }
   ],
   "source": [
    "# read and decode the data\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8').lower()\n",
    "\n",
    "# check the text \n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef697fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# create the vocab \n",
    "vocab = sorted(set(text))\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608251b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding string to numerical value using StringLookUp \n",
    "\n",
    "# create a token first \n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding = 'UTF-8')\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary = list(vocab), \n",
    "    mask_token = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee3e699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[14, 15, 16, 17, 18, 19, 20], [37, 38, 39]]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2598aad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_from_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary = ids_from_chars.get_vocabulary(),\n",
    "    invert = True, \n",
    "    mask_token = None\n",
    ")\n",
    "\n",
    "chars = chars_from_id(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3fa0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_id(ids), axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f346fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for id in ids_dataset.take(10):\n",
    "    print(chars_from_id(id).numpy().decode('utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82077302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'f' b'i' b'r' b's' b't' b' ' b'c' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'b' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'a' b'l' b'l' b':' b'\\n' b's' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'f' b'i'\n",
      " b'r' b's' b't' b' ' b'c' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "# see the sequence\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_id(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c9e3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'first citizen:\\nbefore we proceed any further, hear me speak.\\n\\nall:\\nspeak, speak.\\n\\nfirst citizen:\\nyou '\n"
     ]
    }
   ],
   "source": [
    "# see the actual sentence \n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13d5ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the input and label from the sequence\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f42ee367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Text :  b'first citizen:\\nbefore we proceed any further, hear me speak.\\n\\nall:\\nspeak, speak.\\n\\nfirst citizen:\\nyou'\n",
      "label Text :  b'irst citizen:\\nbefore we proceed any further, hear me speak.\\n\\nall:\\nspeak, speak.\\n\\nfirst citizen:\\nyou '\n"
     ]
    }
   ],
   "source": [
    "dataset_split = sequences.map(split_input_target)\n",
    "\n",
    "for ex_input, ex_target in dataset_split.take(1):\n",
    "    print(\"input Text : \", text_from_ids(ex_input))\n",
    "    print(\"label Text : \", text_from_ids(ex_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f7e9b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the batch and shuffle the dataset\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset_split\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12195c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model variable / parameter\n",
    "\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_unit = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6e86fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GRU(rnn_unit, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7179a063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 40) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0468a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m10,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m40\u001b[0m)          │        \u001b[38;5;34m41,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,989,544</span> (15.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,989,544\u001b[0m (15.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,989,544</span> (15.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,989,544\u001b[0m (15.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b820c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'f the duke is with the soldiers;\\nand for your brother, he was lately sent\\nfrom your kind aunt, duche'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"q&:y3ua'''jlypm&ecfyls!!kphuf$;vzip;h\\npwu.lgr3;o?z\\n$bhh3rog- wwxir\\n&udq'ga!nr?ge&:!f[UNK]ms..vmtqj;b[UNK]l&t\"\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices\n",
    "\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4307054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 40)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(3.6867273, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fe7a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 3s/step - loss: 2.9008 - sparse_categorical_accuracy: 0.2546\n",
      "Epoch 2/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 3s/step - loss: 1.8735 - sparse_categorical_accuracy: 0.4392\n",
      "Epoch 3/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 3s/step - loss: 1.5847 - sparse_categorical_accuracy: 0.5186\n",
      "Epoch 4/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 3s/step - loss: 1.4494 - sparse_categorical_accuracy: 0.5538\n",
      "Epoch 5/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 3s/step - loss: 1.3719 - sparse_categorical_accuracy: 0.5733\n",
      "Epoch 6/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 3s/step - loss: 1.3173 - sparse_categorical_accuracy: 0.5871\n",
      "Epoch 7/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 3s/step - loss: 1.2720 - sparse_categorical_accuracy: 0.5987\n",
      "Epoch 8/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - loss: 1.2302 - sparse_categorical_accuracy: 0.6101\n",
      "Epoch 9/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - loss: 1.1888 - sparse_categorical_accuracy: 0.6219\n",
      "Epoch 10/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 2s/step - loss: 1.1465 - sparse_categorical_accuracy: 0.6345\n",
      "Epoch 11/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 2s/step - loss: 1.1029 - sparse_categorical_accuracy: 0.6477\n",
      "Epoch 12/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 2s/step - loss: 1.0583 - sparse_categorical_accuracy: 0.6615\n",
      "Epoch 13/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 3s/step - loss: 1.0135 - sparse_categorical_accuracy: 0.6763\n",
      "Epoch 14/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 4s/step - loss: 0.9761 - sparse_categorical_accuracy: 0.6872\n",
      "Epoch 15/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 3s/step - loss: 0.9550 - sparse_categorical_accuracy: 0.6931\n",
      "Epoch 16/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 3s/step - loss: 0.9358 - sparse_categorical_accuracy: 0.6979\n",
      "Epoch 17/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 3s/step - loss: 0.9105 - sparse_categorical_accuracy: 0.7063\n",
      "Epoch 18/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 3s/step - loss: 0.8860 - sparse_categorical_accuracy: 0.7135\n",
      "Epoch 19/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 3s/step - loss: 0.8723 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 20/20\n",
      "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 4s/step - loss: 0.8634 - sparse_categorical_accuracy: 0.7186\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6eba6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_id, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_id = chars_from_id\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # create a mask to prevent \"[UNK]\" from being generated\n",
    "\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # put an -inf at each bad index\n",
    "            values = [-float('inf')]*len(skip_ids),\n",
    "            indices = skip_ids,\n",
    "            \n",
    "            # match the size to the vocabulary\n",
    "            dense_shape = [len(ids_from_chars.get_vocabulary())]\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Embedding layer\n",
    "        x = self.model.layers[0](input_ids)\n",
    "        # GRU layer\n",
    "        x = self.model.layers[1](x, initial_state=states)     \n",
    "        # Get the hidden state of the last timestep\n",
    "        states = x[:, -1, :]\n",
    "        # Dense layer\n",
    "        predicted_logits = self.model.layers[2](x)\n",
    "\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_id(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3bfa2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_id, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ea24387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "nay, madam; 'tis a very looker-bady, i\n",
      "\n",
      "mercutio:\n",
      "do not my cold food?\n",
      "\n",
      "tarisan:\n",
      "on both!\n",
      "\n",
      "coriolanus:\n",
      "you are too soon but book.\n",
      "\n",
      "katharina:\n",
      "what appecialling i stand on, when this isles of person, that\n",
      "i am abused king richer of but which here\n",
      "within me; not i pressing him; you are\n",
      "untimelfachery,\n",
      "and much some slower for that father fret,\n",
      "i warrant them. a\n",
      "thought shall watch the proise hurt upon the world;\n",
      "this bond of others to unactouboth!\n",
      "i promise home:\n",
      "for bitter conferer, is something rich.\n",
      "\n",
      "buckingham:\n",
      "you have been so, but at last word in their\n",
      "cames like unto thee, and master crow,\n",
      "hath brought a like untap' time unto your brother;\n",
      "ere i not proud, which too meet borne thin all-a-rubied is;\n",
      "for my tunes look to help with thee.\n",
      "\n",
      "angelo:\n",
      "\n",
      "duke vincentio:\n",
      "on what tongue not? what, ho! where have won 'em;\n",
      "but thomas told me, boy.\n",
      "\n",
      "polixand\n",
      "of greater:\n",
      "help, yese! why, jood slaining kissing, i never chance\n",
      "proclaimended: ongend at his mind;\n",
      "that thou wouldst disphrietes 'that  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 6.9804558753967285\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e9cbee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.get_custom_objects().update({'OneStep': OneStep})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
